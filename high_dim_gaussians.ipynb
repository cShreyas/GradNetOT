{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.optim import Adam\n",
    "from torch.func import vmap, jacrev\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import trange\n",
    "from scipy.stats import wishart\n",
    "\n",
    "from utils.models import mGradNet_C, mGradNet_M, WSoftmax, WTanh\n",
    "from utils.pdfs import MultivariateNormal\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    print(f\"Running for dimension: {dim}\")\n",
    "\n",
    "    np.random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1234)\n",
    "\n",
    "    source_mu = torch.randn(dim,) \n",
    "    source_cov = torch.from_numpy(wishart.rvs(dim+1, np.identity(dim), 1)).float()\n",
    "    source = MultivariateNormal(source_mu, source_cov)\n",
    "    target = MultivariateNormal(torch.zeros(dim), torch.eye(dim))\n",
    "\n",
    "    source_samples = source.sample(1000)\n",
    "    target_samples = target.sample(1000)\n",
    "\n",
    "    def inv_sqrt(A):\n",
    "        eigvals, eigvecs = torch.linalg.eigh(A)\n",
    "        return eigvecs @ torch.diag(eigvals.pow(-0.5)) @ eigvecs.T\n",
    "\n",
    "    A = inv_sqrt(source_cov)\n",
    "    def optimal_ot_map(x):\n",
    "        z = x - source_mu\n",
    "        return z @ A\n",
    "\n",
    "    optimal_target = optimal_ot_map(source_samples).cpu().detach()\n",
    "\n",
    "    # Define the OT map model\n",
    "    for otmap_name in ['mGradNet_C', 'mGradNet_M']:\n",
    "\n",
    "        if otmap_name == 'mGradNet_C':\n",
    "            otmap = mGradNet_C(in_dim=dim, embed_dim=32, num_layers=4, activation=lambda: nn.Tanh())\n",
    "            \n",
    "        elif otmap_name == 'mGradNet_M':\n",
    "            otmap = mGradNet_M(num_modules=4, in_dim=dim, embed_dim=32, activation=lambda: WSoftmax(32))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown OT map type specified.\")\n",
    "\n",
    "\n",
    "        # Train the OT map\n",
    "        otmap = otmap.to(device)\n",
    "        opt = Adam(otmap.parameters(), lr=1e-2)\n",
    "        jacobian_fn = jacrev(otmap)\n",
    "        batched_jacobian_fn = vmap(jacobian_fn)\n",
    "\n",
    "        pbar = trange(1000, dynamic_ncols=True) # train for 1000 iterations\n",
    "        for i in pbar:\n",
    "            opt.zero_grad()\n",
    "            samples = source.sample(1000).to(device)  # sample points from the source distribution\n",
    "            out = otmap(samples)\n",
    "        \n",
    "            J = batched_jacobian_fn(samples)\n",
    "            J = J.squeeze(1)\n",
    "            \n",
    "            _, logabsdet = torch.linalg.slogdet(J)\n",
    "\n",
    "            log_p = source.log_pdf(samples)       # source log-density\n",
    "            log_q = target.log_pdf(out)           # target log-density\n",
    "\n",
    "            loss = F.l1_loss(logabsdet, log_p - log_q)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(otmap.parameters(), max_norm=2.0) # clip gradient norms for stability\n",
    "            opt.step()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {loss.item():.4f}, Test loss: {F.mse_loss(otmap(source_samples), optimal_target).item():.4f}\")\n",
    "\n",
    "        results[(otmap_name, dim)] = F.mse_loss(otmap(source_samples), optimal_target).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "dims = [2, 4, 8, 16, 32, 64, 128]\n",
    "dim_strs = [str(dim) for dim in dims]\n",
    "otmap_name = 'mGradNet_C'\n",
    "mses = [results[(otmap_name, dim)] for dim in dims]\n",
    "ax.bar(np.arange(len(dims))-0.2, mses, width=0.4, label='mGradNet-C')\n",
    "otmap_name = 'mGradNet_M'\n",
    "mses = [results[(otmap_name, dim)] for dim in dims]\n",
    "ax.bar(np.arange(len(dims))+0.2, mses, width=0.4, label='mGradNet-M')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_xticks(np.arange(len(dims)))\n",
    "ax.set_xticklabels(dim_strs)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend()\n",
    "ax.set_title('MSE between Learned and Optimal OT Maps')\n",
    "plt.show()\n",
    "plt.savefig('high_dim_gaussians_results.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
